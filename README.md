# etl-volumetria-pyspark-parquet

**Pipeline ETL modular em PySpark para processar grandes volumes de dados**  
Leitura de CSV, transformaÃ§Ã£o de dados, particionamento e escrita em Parquet com boas prÃ¡ticas de engenharia.

---

## ğŸ› ï¸ Tecnologias

- **PySpark** â€“ processamento distribuÃ­do  
- **Parquet** â€“ formato colunar particionado por data  
- **Python 3.8+** â€“ estrutura de projeto, configuraÃ§Ã£o e testes  
- **pytest** â€“ testes unitÃ¡rios das transformaÃ§Ãµes  

---

## ğŸš€ Funcionalidades

1. **IngestÃ£o**: lÃª arquivos CSV de alto volume (â‰¥1â€¯GB)  
2. **TransformaÃ§Ã£o**: limpeza de dados, casting de tipos, enrich de colunas e deduplicaÃ§Ã£o  
3. **Carga**: gravaÃ§Ã£o em Parquet particionado (`data_pedido`)  
4. **Modularidade**:  
   - `src/config.py` â†’ SparkSession e settings  
   - `src/ingest.py` â†’ leitura dos dados  
   - `src/transform.py` â†’ lÃ³gica de limpeza e enriquecimento  
   - `src/load.py` â†’ escrita particionada  
   - `src/main.py` â†’ orquestraÃ§Ã£o do pipeline  
5. **Boas prÃ¡ticas**: configuraÃ§Ã£o via `.env`, logging estruturado, testes unitÃ¡rios e diagrama de fluxo

---

## ğŸ’¡ Como rodar

1. Clone o repositÃ³rio  
   ```bash
   git clone https://github.com/seu-usuario/etl-volumetria-pyspark-parquet.git
   cd etl-volumetria-pyspark-parquet
