# etl-volumetria-pyspark-parquet

**Pipeline ETL modular em PySpark para processar grandes volumes de dados**  
geraÃ§Ã£o de dados sinteticos, transformaÃ§Ã£o de dados, particionamento e escrita em Parquet com boas prÃ¡ticas de engenharia de dados.

---

## ğŸ› ï¸ Tecnologias

- **PySpark** â€“ processamento distribuÃ­do  
- **Parquet** â€“ formato colunar particionado por data  
- **Python 3.7+** â€“ estrutura de projeto, configuraÃ§Ã£o e testes  
- **pytest** â€“ testes unitÃ¡rios das transformaÃ§Ãµes  

---

## ğŸš€ Funcionalidades

1. **GeraÃ§Ã£p de dados_sinteticos**: GeraÃ§Ã£o dados de alto volume (â‰¥1â€¯GB)
2. **IngestÃ£o**: lÃª arquivos parquet de alto volume (â‰¥1â€¯GB)
3. **TransformaÃ§Ã£o**: limpeza de dados, casting de tipos, enrich de colunas e deduplicaÃ§Ã£o  
4. **Carga**: gravaÃ§Ã£o em Parquet particionado (`data_pedido`)  
5. **Modularidade**:  
   - `src/config.py` â†’ SparkSession e settings
   - `src/Dado_sinteticos_SOR.py` â†’ GeraÃ§Ã£o dados sinteticos
   - `src/ingest.py` â†’ leitura dos dados  
   - `src/transform.py` â†’ lÃ³gica de limpeza e enriquecimento  
   - `src/load.py` â†’ escrita particionada  
   - `src/main.py` â†’ orquestraÃ§Ã£o do pipeline  
6. **Boas prÃ¡ticas**: configuraÃ§Ã£o via `.env`, logging estruturado, testes unitÃ¡rios e diagrama de fluxo

---

## ğŸ’¡ Como rodar

1. Clone o repositÃ³rio  
   ```bash
   git clone https://github.com/seu-usuario/etl-volumetria-pyspark-parquet.git
   cd etl-volumetria-pyspark-parquet
