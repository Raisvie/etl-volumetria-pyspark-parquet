# etl-volumetria-pyspark-parquet

**Pipeline ETL modular em PySpark para processar grandes volumes de dados**  
geração de dados sinteticos, transformação de dados, particionamento e escrita em Parquet com boas práticas de engenharia de dados.

---

## 🛠️ Tecnologias

- **PySpark** – processamento distribuído  
- **Parquet** – formato colunar particionado por data  
- **Python 3.7+** – estrutura de projeto, configuração e testes  
- **pytest** – testes unitários das transformações
